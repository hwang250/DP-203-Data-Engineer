{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z40SJW9I9Rh9"
      },
      "source": [
        "# Knowledge Distillation\n",
        "Created in PyTorch by [Laia Tarrés](https://www.linkedin.com/in/laia-tarres-9a5369138) for the [Postgraduate Course in Artificial Intelligence with Deep Learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) ([UPC School](https://www.talent.upc.edu/ing/), 2021).\n",
        "\n",
        "Updated by [Gerard I. Gállego](https://www.linkedin.com/in/gerard-gallego/).\n",
        "\n",
        "*Based on other notebooks that use distillation [1](https://colab.research.google.com/github/sayakpaul/Knowledge-Distillation-in-Keras/blob/master/Distillation_with_Transfer_Learning.ipynb#scrollTo=b1jE623hh781), [2](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/knowledge_distillation.ipynb), [3](https://colab.research.google.com/drive/1-yHSQTljXyca2aSFhpM2y4n9B4M-KWso#scrollTo=3JApQdNz19bT), [4](https://koushik-nov01.medium.com/knowledge-distillation-with-pytorch-40febcf77440) for educational purposes.*\n",
        "\n",
        "Modern state-of-the-art neural network architectures are HUGE.\n",
        "\n",
        "Unfortunately, more is sometimes not better when it comes to the number of parameters. Sure, more parameters seem to mean better results, but also massive computational costs.\n",
        "\n",
        "However, deploying much smaller models can also present a significant challenge for machine learning engineers. In practice, small and fast models are much better than massive ones.\n",
        "\n",
        "Because of this, researchers and engineers have put significant energy into compressing models.\n",
        "\n",
        "To optimize these costs by compressing the models, three main methods have emerged:\n",
        "\n",
        "*   Weight pruning\n",
        "*   Quantization\n",
        "*   knowledge distillation\n",
        "\n",
        "\n",
        "Today we will focus on Knowledge Distillation. Knowledge Distillation is a procedure for model compression, in which a small (student) model is trained to match a large pre-trained (teacher) model.\n",
        "\n",
        "Knowledge is transferred from the teacher model to the student by minimizing a loss function, aimed at matching the teacher outputs as well as ground-truth labels.\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "- [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC8B-UAzFTVG"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fEH667VNX_Sa",
        "outputId": "36802c38-fadc-4429-8b9f-0be7a34b7c6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tJlH25EvX_Sb"
      },
      "outputs": [],
      "source": [
        "# Necessary imports\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o63CO4z5Fr67"
      },
      "source": [
        "Define hyerparameters, and remember to set the runtime type accelerator as GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5C3s8wASX_Sb"
      },
      "outputs": [],
      "source": [
        "hparams = {\n",
        "    'batch_size':32,\n",
        "    'num_epochs':3,\n",
        "    'num_classes':10,\n",
        "    'learning_rate':1e-4,\n",
        "    'learning_rate_dist': 5e-3,\n",
        "    'log_interval':2000,\n",
        "}\n",
        "hparams['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "assert(hparams['device']=='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGllOagRFYAn"
      },
      "source": [
        "# Define MNIST dataset and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mELWPxwLX_Sb"
      },
      "outputs": [],
      "source": [
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"dataset/\",\n",
        "    train=True,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"dataset/\",\n",
        "    train=False,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "# Create train and test dataloaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=hparams['batch_size'], shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WKDBeUOGBit"
      },
      "source": [
        "# Define the Teacher Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhUhwIFdX_Sc"
      },
      "outputs": [],
      "source": [
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=10):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=64,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=64,\n",
        "            out_channels=256,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.fc1 = nn.Linear(256 * 7 * 7, hparams['num_classes'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hxjn97tGPrY"
      },
      "source": [
        "## Exercise 1: Declare the teacher model, and list the number of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ED8NqiSX_Sc"
      },
      "outputs": [],
      "source": [
        "# TODO: Declare the teacher model\n",
        "teacher_model = ...\n",
        "# TODO: List its parameters, given an input of [bs, nchannels, width, depth], using the summary function from torchinfo\n",
        "summary(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heORRbM6HNs5"
      },
      "source": [
        "# Define the student model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r22auZ7cX_Sc"
      },
      "outputs": [],
      "source": [
        "class StudentModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=10):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=8,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=8,\n",
        "            out_channels=16,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrxgujRVJ1lo"
      },
      "source": [
        "## Exercise 2: Declare the student model, and list the number of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jgQSkrCX_Sd"
      },
      "outputs": [],
      "source": [
        "# TODO: Declare the teacher model\n",
        "student_model = ...\n",
        "summary(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_09kqJbqHcWG"
      },
      "source": [
        "Let's define a helper function that computes the accuracy and the number of correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CceSZkTX_Sd"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model, device):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    return (num_correct/num_samples).item()\n",
        "\n",
        "def correct_predictions(predicted_batch, label_batch):\n",
        "  pred = predicted_batch.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "  acum = pred.eq(label_batch.view_as(pred)).sum().item()\n",
        "  return acum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma_kQc6mHksV"
      },
      "source": [
        "Let's define a basic training pipeline for any network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkvhC8UqKH7g"
      },
      "source": [
        "#Exercise 3: define the basic training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Piox05qfX_Sd"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        device = hparams['device']\n",
        "        model.to(device)\n",
        "\n",
        "        pbar = tqdm(train_loader, total=len(train_loader), position=0, leave=True, desc=f\"Epoch {epoch}\")\n",
        "        for data, targets in pbar:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # TODO: forward method\n",
        "            scores = ...\n",
        "            loss = ...\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # TODO: backward pass\n",
        "            loss...\n",
        "            optimizer...\n",
        "            optimizer...\n",
        "\n",
        "        avg_loss = sum(losses) / len(losses)\n",
        "        acc = check_accuracy(test_loader, model, device)\n",
        "        print(f\"Loss:{avg_loss:.2f}\\tAccuracy:{acc:.2f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_model(model, test_loader):\n",
        "  model.eval()\n",
        "  device = hparams['device']\n",
        "  eval_loss = 0\n",
        "  acc = 0\n",
        "  logsoftmax = nn.LogSoftmax(dim=-1)\n",
        "  beg_t = timer()\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          output = logsoftmax(model(data))\n",
        "          # compute number of correct predictions in the batch\n",
        "          acc += correct_predictions(output, target)\n",
        "  # Average acc across all correct predictions batches now\n",
        "  end_t = timer()\n",
        "  train_time = end_t - beg_t\n",
        "  test_acc = 100. * acc / len(test_loader.dataset)\n",
        "  print('Test set:  Accuracy: {}/{} ({:.0f}%) Time to test: {} seconds.'.format(\n",
        "       acc, len(test_loader.dataset), test_acc, round(train_time, 2),\n",
        "      ))\n",
        "  return test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlVyZ4voPKXz"
      },
      "source": [
        "##Train the teacher model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg0GzPtrX_Sd"
      },
      "outputs": [],
      "source": [
        "# TODO: Declare the teacher model, criterion and optimizer\n",
        "teacher_model = ...\n",
        "criterion = nn...\n",
        "optimizer = torch.optim.Adam(..., ...)\n",
        "\n",
        "teacher_model = train_model(teacher_model, criterion, optimizer, train_loader, epochs=hparams['num_epochs'])\n",
        "test_model(teacher_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIClVGA_JkfI"
      },
      "source": [
        "# Exercise 4: Perform knowledge distillation (transfer knowledge from the teacher to the student)\n",
        "\n",
        "In this example, we have two losses that are combined to obtain the loss that will be backpropagated in order to train the student.\n",
        "\n",
        "We have:\n",
        "\n",
        "\n",
        "*   **Classification loss (student loss)**: the typical loss: in this case, the network is outputing right before the softmax. We apply CrossEntropyLoss.\n",
        "*   **Distillation loss**: in this loss, we are comparing the softened outputs from the softmax. As the model is outputing right before the softmax, we will have to apply the softmax with the corresponding temperature term and then MSELoss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXaipIiQX_Se"
      },
      "outputs": [],
      "source": [
        "def train_step(teacher, student, optimizer, classification_loss_fn, distillation_loss_fn, temp, alpha, epoch, device):\n",
        "    losses = []\n",
        "    pbar = tqdm(train_loader, total=len(train_loader), position=0, leave=True, desc=f\"Epoch {epoch}\")\n",
        "    device = hparams['device']\n",
        "\n",
        "    for data, targets in pbar:\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # TODO: Compute teacher soft predictions\n",
        "            teacher_preds = ...\n",
        "\n",
        "        # TODO: Get the student soft targets and compute the classification loss between\n",
        "        student_preds = ...\n",
        "\n",
        "        # TODO: compute the classification loss\n",
        "        student_loss = ...\n",
        "        # TODO: Compute the distillation loss. Remember, that we are comparing the outputs of the softmax for both predictions\n",
        "        distillation_loss = ...( F.softmax(... / temp, dim=1), F.softmax(.../ temp, dim=1) )\n",
        "\n",
        "        loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # TODO: backward pass and update optimizer.\n",
        "        loss...\n",
        "        optimizer...\n",
        "        optimizer...\n",
        "\n",
        "    avg_loss = sum(losses) / len(losses)\n",
        "    return avg_loss\n",
        "\n",
        "def train_distillation(teacher, student, optimizer, classification_loss_fn, distillation_loss_fn, epochs, temp=7, alpha=0.3, device='cuda'):\n",
        "    device = hparams['device']\n",
        "    teacher = teacher.to(device)\n",
        "    student = student.to(device)\n",
        "    teacher.eval()\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_step(\n",
        "            teacher,\n",
        "            student,\n",
        "            optimizer,\n",
        "            classification_loss_fn,\n",
        "            distillation_loss_fn,\n",
        "            temp,\n",
        "            alpha,\n",
        "            epoch,\n",
        "            device\n",
        "        )\n",
        "        acc = check_accuracy(test_loader, student, device)\n",
        "        print(f\"Loss:{loss:.2f}\\tAccuracy:{acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGFKy-4kT-ZV"
      },
      "source": [
        "Let's check what temperature scaling is doing, and how it is \"flattening\" the outputs of the softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL4p0l_iX_Se"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "logits=np.array([1.,2.,3.,-1.])\n",
        "print(f'Logits: {logits}')\n",
        "logits_exp = np.exp(logits)\n",
        "print(f'Logits exp: {logits_exp}')\n",
        "logits_exp_normalized = np.exp(logits)/sum(np.exp(logits)) #this would be like applying the softmax\n",
        "print(f'Logits exp normalized: {logits_exp_normalized}')\n",
        "\n",
        "#Let's try with a few values of T:\n",
        "T = [1.,5.,7.,10.]\n",
        "\n",
        "for t in T:\n",
        "  logits_exp_normalized_t = np.exp(logits/t)/sum(np.exp(logits/t))\n",
        "  print(f'Temperature[{t}] - {logits_exp_normalized_t}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCa3rGmWV1Z3"
      },
      "source": [
        "##Exercise 5: call the distillation function\n",
        "\n",
        "You should declare the two types of losses:\n",
        "\n",
        "\n",
        "1.   The appropiate for classification, when the model doesn't have a last activation layer. Categorical Cross entropy is recommended.\n",
        "2.   The appropiate for distillation, which will be able to compare between softened outputs of the softmax. MSE is recommended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFWgeZNIX_Se"
      },
      "outputs": [],
      "source": [
        "# TODO: declare what you need to call the train_distillation function\n",
        "student_model = ...\n",
        "classification_loss_fn = nn...\n",
        "distillation_loss_fn = nn...\n",
        "\n",
        "# TODO: Declare the optimizer\n",
        "optimizer = torch.optim.Adam(...,...)\n",
        "\n",
        "train_distillation(teacher_model, student_model, optimizer, classification_loss_fn, distillation_loss_fn, epochs=hparams['num_epochs'], temp=6, alpha=0.2, device = hparams['device'])\n",
        "test_model(student_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmTiMgfFI2wb"
      },
      "source": [
        "##Exercise 6: For comparison, let's train the student model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BXeKtsEX_Se"
      },
      "outputs": [],
      "source": [
        "# TODO: declare a new student model\n",
        "student_model= ....to(hparams['device'])\n",
        "criterion = nn...\n",
        "optimizer = torch.optim.Adam(..., ...)\n",
        "\n",
        "student_model = train_model(student_model, criterion, optimizer, train_loader, hparams['num_epochs'])\n",
        "test_model(student_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Bo0ixWUWAn"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "Yay! You have seen a didactic method of how to implement distillation. Although you probably haven't seen huge improvements in terms of accuracy, check how faster the student model is from the teacher when doing inference (test). And imagine how big of an impact that has when we are working with huge networks and with way bigger datasets than MNIST.\n",
        "\n",
        "There are many uses for Distillation, but one of the most impactful have been DistilBERT, a distilled version of the famous BERT transformer.\n",
        "\n",
        "#Extra:\n",
        "\n",
        "Do some further experiments with distillation, which combination gives you better results when varying different values for:\n",
        "\n",
        "*   temp\n",
        "*   alpha\n",
        "*   loss for distillation (note that we are using mse to compare the outputs of the softmax scaled, but we could also use divergence_loss_fn to compare the outputs of the log_softmax, among other loss functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eDroO4MX_Sf"
      },
      "outputs": [],
      "source": [
        "# TODO: declare what you need to call the train_distillation function\n",
        "student_model = ...\n",
        "student_loss_fn = ...\n",
        "mse_loss_fn = ...\n",
        "#divergence_loss_fn = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
        "optimizer = ...\n",
        "\n",
        "train_distillation(teacher_model, student_model, optimizer, student_loss_fn, mse_loss_fn, epochs=hparams['num_epochs'], temp=..., alpha=..., device = hparams['device'])\n",
        "test_model(student_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqxlA5mmFqMi"
      },
      "source": [
        "# Some cool Examples where they use distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX7MPQcRQzYI"
      },
      "source": [
        "## DistilBert\n",
        "In the following example, you can experiment with one of the most famous aplications of distillation: DistilBERT. In this [paper](https://arxiv.org/abs/1910.01108) they proved that they could use a smaller version of the model with fewer parameters and less computational resources.\n",
        "\n",
        "For comparison:\n",
        "\n",
        "BERT had 110 million parameters, and has 668 inference time.\n",
        "DistilBERT had 60 million parameters and has 410s inference time.\n",
        "\n",
        "That is, reducing 40% the number of parameters and the network being faster without losing performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQXxn_VOX_Sf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "!pip install -q transformers datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i24c0IyQX_Sf"
      },
      "outputs": [],
      "source": [
        "distilbert_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def get_sents_representations(sents):\n",
        "    encoded_input = distilbert_tokenizer(sents, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    distilbert_output = distilbert(**encoded_input)[0]\n",
        "    sentence_repr = distilbert_output[:, 0]\n",
        "\n",
        "    return distilbert_output, sentence_repr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2PskOpTX_Sf"
      },
      "outputs": [],
      "source": [
        "#@title  { run: \"auto\", vertical-output: true }\n",
        "\n",
        "#@markdown Show 5 DistilBERT sentence representations to 2-D\n",
        "sentence_1 = \"Hello, my name is Joe\" #@param {type:\"string\"}\n",
        "sentence_2 = \"Hi, I'm Joey\" #@param {type:\"string\"}\n",
        "sentence_3 = \"Goodbye, see you at 5pm\" #@param {type:\"string\"}\n",
        "sentence_4 = \"Bye, see you later\" #@param {type:\"string\"}\n",
        "sentence_5 = \"Attention is All You Need\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "sentences = [sentence_1, sentence_2, sentence_3, sentence_4, sentence_5]\n",
        "\n",
        "distilbert_output, sentence_repr = get_sents_representations(sentences)\n",
        "\n",
        "print(f\"DistilBERT output: {distilbert_output.shape}\")\n",
        "print(f\"Sentence representations: {sentence_repr.shape}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "sentence_repr_2d = pca.fit_transform(sentence_repr.detach().numpy())\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.scatter(sentence_repr_2d[:,0], sentence_repr_2d[:,1])\n",
        "plt.title(\"Sentence representations (PCA projection)\")\n",
        "plt.xlim(sentence_repr_2d[:,0].min() - 1, sentence_repr_2d[:,0].max() + 4)\n",
        "plt.ylim(sentence_repr_2d[:,1].min() - 1, sentence_repr_2d[:,1].max() + 1)\n",
        "\n",
        "for x, y, s in zip(sentence_repr_2d[:,0], sentence_repr_2d[:,1], sentences):\n",
        "    plt.text(x+0.15, y+0.15, s)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRP1fWn2RpwQ"
      },
      "source": [
        "## TinyGAN\n",
        "In the following example, you can experiment with a computer-vision related application: GANS.\n",
        "\n",
        "one of the most famous aplications of distillation: DistilBERT. In this [paper](https://arxiv.org/abs/1910.01108) they proved that they could use a smaller version of the model with fewer parameters and less computational resources.\n",
        "\n",
        "For comparison:\n",
        "\n",
        "*   BigGAN had 50.1 million parameters for the Generator, that performed 8.32 flops.\n",
        "*   TinyGAN had 3.1 million parameters for the Generator, that performed 0.44 flops.\n",
        "\n",
        "\n",
        "That is, using a model that is 16 times smaller without loosing performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6acAmxcX_Sg"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/terarachang/ACCV_TinyGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgenayI3X_Sg"
      },
      "outputs": [],
      "source": [
        "cd ACCV_TinyGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9aUTpB1X_Sg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ9TemjMX_Sg"
      },
      "outputs": [],
      "source": [
        "from model import Generator\n",
        "from utils import *\n",
        "G = Generator(image_size=128, conv_dim=32, z_dim=128, c_dim=128, repeat_num=5)\n",
        "restore_model(30, 'gan/models', G, None, None, None)\n",
        "G.to(device)\n",
        "G.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNLZ770eS4GY"
      },
      "source": [
        "Run this two cells as many times as you would like, to see different results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EG0rKitX_Sg"
      },
      "outputs": [],
      "source": [
        "z_dim = 128\n",
        "n_row = 5\n",
        "n_samples = n_row * n_row\n",
        "noise = torch.FloatTensor(truncated_normal(n_samples*z_dim)) \\\n",
        "\t\t\t\t\t\t\t\t\t\t.view(n_samples, z_dim).to(device)\n",
        "\n",
        "label = np.random.choice(398, n_row, replace=False) # sample from all animal classes\n",
        "print(label)\n",
        "label_t = torch.tensor(label).repeat(n_row).to(device)\n",
        "\n",
        "#get the 5 predictions prediction conditioned to the label for 5 samples\n",
        "with torch.no_grad():\n",
        "  out = G(noise, label_t).detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbtgyVeWX_Sg"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "from IPython.display import Image\n",
        "save_image(denorm(out), 'demo.png', nrow=n_row)\n",
        "Image(filename='demo.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}